{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST VERİ SETİ\n",
        "0'dan 9'a kadar olan rakamların el yazısı görselleri."
      ],
      "metadata": {
        "id": "eERRKr8oOn0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "AZYqqjv1MHv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3140535e-9340-4372-d0b3-e7c323b27167"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape #array'in biçimini gösteriyor\n",
        "# genel olarak (sample, feature) formatında oluyor\n",
        "# resimler için (sample, height, width, color_depth)\n",
        "# bu eğitim görselleri için: 60000 fotoğraf, 28x28 boyutunda\n",
        "# video için (samples, frames, height, width, color_depth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMOR_XZZMWtG",
        "outputId": "4b7072f7-57b6-485f-a5c5-fe3b4ed9b676"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_labels) # len ile adet \"sample/örnek\" var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwk-GoxjMZ50",
        "outputId": "c0f0c375-5f1a-4dbc-c20c-b8c85e081c1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels # görsellerde hangi resimler olduğu, nasıl \"etiketlendiği\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwiZhGluMdJF",
        "outputId": "138218c0-5c33-4287-9db4-0a7d16fb70de"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_images.shape # 10000 adet 28x28 test görseli"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PivghQFqMeq9",
        "outputId": "781ee0f3-a2f6-4c5c-da11-c44c7c41c8a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S7n3TZDMhHk",
        "outputId": "00c010f3-eb1c-4f3f-c2ad-4b7e0caac880"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x03hfzD5MjQT",
        "outputId": "ae3753b5-fd33-4c51-c96a-5a5d0378d789"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras # keras, deep learning framework'ü\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential([ # katmanlar oluşturuyoruz\n",
        "    layers.Dense(512, activation=\"relu\"), #units kaç nöron olacağı/çıkış boyutu\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "CYLo0uDPMkhg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\", #gerçek değer - tahmin\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "sljqJdbHM1Ja"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "# siyah 0, beyaz 255'tir fakat biz bunu normalleştirip daha ufak sayılarla\n",
        "# uğraşmak isteriz. bu sebeple 255'e bölerek bunu 0-1 arasında sınırlıyoruz.\n",
        "# float'a çevrilme sebebi matris hesaplamalarında ve gradient descent'te\n",
        "# daha doğru sonuçlar alabilmek.\n",
        "\n",
        "# (60000, 28*28) yaparak bunu (60000, 784) haline çeviriyoruz.\n",
        "# Çünkü Dense düzleştirilmiş (flatten) vektörlerle çalışıyor bu yüzden genelde\n",
        "# 2D vektörler verilir. Biz de bunun için 2D vektör haline getirdik."
      ],
      "metadata": {
        "id": "GXITnj5ZNHbk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm4OQX_ZM-80",
        "outputId": "12a41f2e-e4ab-4248-b2b8-b12b8cb0dd1b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.1690 - loss: 2.2823\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.5451 - loss: 1.9064\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.7125 - loss: 1.1962\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.7924 - loss: 0.8250\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.8287 - loss: 0.6489\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b3df7033ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_digits = test_images[0:10] # ilk 10 örnek 0-9\n",
        "predictions = model.predict(test_digits) # ilk 10 örnek için tahminler oluştu.\n",
        "predictions[0] # 0 indeksindeki (ilk) örnek için tahminler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVrhizThNcMx",
        "outputId": "4af24002-31c8-4ca9-99c0-614e9111b719"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.8977620e-04, 4.5605964e-05, 1.0063551e-04, 1.2872067e-03,\n",
              "       1.7258996e-03, 2.5792336e-03, 9.4219286e-06, 9.6636724e-01,\n",
              "       3.9739686e-04, 2.6897451e-02], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0].argmax() # ilk örnek için tahminlerden en yükseği"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fj8c5D4OLqr",
        "outputId": "eaf74bcf-0fd0-4f1a-8fe7-937b89c919d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(7)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0][7] # kaç oranla 7 olduğunu tahmin ettiği"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c02azJmEORds",
        "outputId": "c7e5932c-0fd9-4dfd-f190-66d8e4679244"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(0.96636724)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels[0] # gerçekte etiketinde ne olduğu (o da 7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gni4FzIjOSo-",
        "outputId": "907325b1-d2d2-4c19-fd53-cb905f877f52"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.uint8(7)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"test_acc: {test_acc}\")\n",
        "print(f\"test_loss: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lj43yY9OUil",
        "outputId": "a8a9d77d-1d6b-42f9-db6d-85d42b6e9184"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8273 - loss: 0.6133\n",
            "test_acc: 0.848800003528595\n",
            "test_loss: 0.5552056431770325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SİNİR AĞLARI İÇİN VERİ GÖSTERİMİ"
      ],
      "metadata": {
        "id": "SPQ7_739Ol41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scalar -> tek boyutlu vektör, rank(0)\n",
        "import numpy as np\n",
        "x = np.array(12)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5_KOw3COwja",
        "outputId": "f7672526-3872-4f65-d6fe-2273df87ad54"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(12)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.ndim # kaç boyutlu olduğu, skalar olduğu için 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D5IQaVQPGfY",
        "outputId": "8951b273-878a-4f22-f633-c7f7a339ab33"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"vektör\" rank 1 tensors\n",
        "x = np.array([12, 3, 6, 14, 7])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTbj3mwEPHV9",
        "outputId": "4d299eb7-74d5-4693-ed3a-2eb4c62e5aa7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12,  3,  6, 14,  7])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.ndim # 5 adet verisi var. 5D vector ama 1D tensor,\n",
        "#her bir [] tensör diyebiliriz."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qtiG3qWP2zF",
        "outputId": "8d2a6b28-9af3-4cd3-f4a4-e09214e7a047"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# matris -> 2D tensors\n",
        "x = np.array([[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35 ,1],\n",
        "              [7, 80, 4, 36, 2]])\n",
        "x.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2zc04cEP38a",
        "outputId": "8c3c297f-645f-463d-baed-6ae001f8d869"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rank 3 -> 3D tensors\n",
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35 ,1],\n",
        "              [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35 ,1],\n",
        "              [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35 ,1],\n",
        "              [7, 80, 4, 36, 2]]])\n",
        "\n",
        "x.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wf2pAgwQn0j",
        "outputId": "96c36e84-f635-4f87-f27f-f4ce1983f083"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "digit = train_images[4]\n",
        "plt.imshow(digit, cmap=plt.cm.binary) #cmap: colormap. binary -> siyah/beyaz\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "p2xmfTwJQwo3",
        "outputId": "f2d07a9b-ea10-4fc6-c86b-b489cc4ff1da"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mM__eCCSl6O",
        "outputId": "0e199442-dd89-4410-9b98-4ffe8c5ce06c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.uint8(9)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_slice = train_images[10:100]\n",
        "my_slice.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b40zceP5S1Ge",
        "outputId": "ceaadace-d2c7-4d3d-fe32-90af90558190"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_slice = train_images[10:100, : , : ]\n",
        "my_slice.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxF-fTKAS_N0",
        "outputId": "23ca1c8b-8726-4de1-ab1b-47d1acfd1bac"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_slice = train_images[10:100, 0:28, 0:28]\n",
        "my_slice.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "387FjV-KTH3f",
        "outputId": "6c51df17-4824-4504-bc9e-612c68638e08"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_slice = train_images[:, 14:, 14:]\n",
        "my_slice.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgHMQC8OTOA_",
        "outputId": "b468b005-aac7-4d55-f65b-7b00de34eadf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 14, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_digit = train_images[4, 7:-7, :-7]\n",
        "plt.imshow(train_digit)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "24Q9mDxITYr-",
        "outputId": "c657b937-654b-4ac2-a6c8-d1a0288e4dfe"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BATCH"
      ],
      "metadata": {
        "id": "vW6flsS7UCaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batchler bir gruplar, model genel olarak datasetini tek seferde işlemez, böler"
      ],
      "metadata": {
        "id": "CxBTiDLNUE9i"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = train_images[:128] # ilk 128'lik kısım\n",
        "batch = train_images[128:255] #2. 128'lik kısım"
      ],
      "metadata": {
        "id": "hUozR-z8UN8H"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5j0ndt8UUy8",
        "outputId": "749789e7-04bf-4a46-aa12-2daf787ac862"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(127, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n. batch\n",
        "n = 3\n",
        "batch = train_images[128 * n: 128 * (n+1)]\n",
        "batch.shape\n",
        "\n",
        "\n",
        "# veri setini 128 128 olarak bölüyoruz\n",
        "# kaçıncı 128'lik bölümü aradığımızı bu şekilde buluyoruz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbQlfJH3UXur",
        "outputId": "ff822636-1cdf-477e-a0d0-15857d3cd8a1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# elle relu yazma\n",
        "ReLU: 0'dan önceki değerler 0, 0'dan itibaren lineer olark artıyor"
      ],
      "metadata": {
        "id": "-8wNebSQcbC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([\n",
        "    [-1, 2, -3],\n",
        "    [4, -5, 6]\n",
        "])"
      ],
      "metadata": {
        "id": "fCxfJmeeccTq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_relu(x):\n",
        "  assert len(x.shape) == 2\n",
        "  x = x.copy()\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i, j] = max(x[i,j], 0) # 0 ile karşılaştırıyor, negatif ise 0 alıyor\n",
        "  return x\n",
        "\n",
        "naive_relu(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8NAW2BtciHK",
        "outputId": "d4599582-2b94-4ce7-9c04-846e10ec3dfd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 2, 0],\n",
              "       [4, 0, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.array([\n",
        "    [2, -4, 7],\n",
        "    [-2, 9, -1]\n",
        "])\n",
        "\n",
        "# toplama\n",
        "\n",
        "def naive_add(x, y):\n",
        "  assert len(x.shape) == 2\n",
        "  assert x.shape == y.shape\n",
        "  x = x.copy()\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i ,j] += y[i, j]\n",
        "\n",
        "  return x\n",
        "\n",
        "print(X)\n",
        "print(Y)\n",
        "naive_add(X,Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAf24Sw_c3Sx",
        "outputId": "c5fa0727-c719-43b2-d792-1e4b3426b580"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1  2 -3]\n",
            " [ 4 -5  6]]\n",
            "[[ 2 -4  7]\n",
            " [-2  9 -1]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1, -2,  4],\n",
              "       [ 2,  4,  5]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# döngülerle yazmak ile matris yöntemi arasındaki zaman farkı\n",
        "\n",
        "import time\n",
        "x = np.random.random((20,100))\n",
        "y = np.random.random((20,100))\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "  z = x + y\n",
        "  z = np.maximum(z, 0.)\n",
        "  print(\"Took: {0:.2f} s\".format(time.time() - t0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EAgleWve8ig",
        "outputId": "89f3a246-b08b-4709-db1b-d9f16074a82b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "  z = naive_add(x,y)\n",
        "  z = naive_relu(z)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8fV5oQKfhBX",
        "outputId": "8c51680e-e43b-4bd9-b36f-58595d0eb506"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 1.86 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Broadcast\n",
        "\n",
        "farklı shape'e ve boyuta sahip tensörlerde işlem yapmak. 2D bir tensor ile 1D vektör toplanacaksa, vektör tüm 2D elemanlarına uygulanır (broadcast edilir).\n"
      ],
      "metadata": {
        "id": "Chrigm1oggYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.random.random((32, 10)) # 32 x 10 matris\n",
        "y = np.random.random((10,)) # 10 x 1 vector\n",
        "print(f\"x_dim : {x.ndim} and x_shape : {x.shape}\\ny_dim : {y.ndim} and y_shape : {y.shape}\")\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw6T3bZ2f1WZ",
        "outputId": "14bfc1dd-aec4-479e-9cc5-6ab5bce10667"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_dim : 2 and x_shape : (32, 10)\n",
            "y_dim : 1 and y_shape : (10,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "       0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.expand_dims(y, axis = 0)\n",
        "print(f\"y_dim : {y.ndim} and y_shape : {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34565VWTg0Lo",
        "outputId": "16a507bb-a944-4462-d90b-b822c4c762dd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_dim : 2 and y_shape : (1, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDghtQyfhF7n",
        "outputId": "7b2088c7-0d7a-4119-db41-3d593c658d87"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.concatenate([y] * 32, axis=0)\n",
        "print(f\"y_dim : {y.ndim} and y_shape : {y.shape}\")\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I_9VYr2g0Z6",
        "outputId": "ca2fb498-b3d4-4d86-ea55-28ac40e0405d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_dim : 2 and y_shape : (32, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615],\n",
              "       [0.35797921, 0.41214668, 0.66052297, 0.91704308, 0.53350239,\n",
              "        0.02771112, 0.60856278, 0.52241871, 0.73281581, 0.56373615]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_add_matrix_and_vector(x,y):\n",
        "  assert len(x.shape) == 2\n",
        "  assert len(y.shape) == 1\n",
        "  x = x.copy()\n",
        "\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i, j] += y[i, j]\n",
        "  return x"
      ],
      "metadata": {
        "id": "kZhjNgDSiRU3"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# peki hangi değişken broadcast yapılmasına daha uygun\n",
        "\n",
        "x = np.random.random((64, 3, 32, 10))\n",
        "y = np.random.random((32,10))\n",
        "z = np.maximum(x,y)\n",
        "print(z.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1EUFtrh2KyF",
        "outputId": "5e397dbb-ed68-44d3-f0a4-f041261fee6a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 3, 32, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tensor çarpımı -> nokta çarpımı\n"
      ],
      "metadata": {
        "id": "Eck87hVn2rF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.random((32,))\n",
        "y = np.random.random((32,))\n",
        "z = np.dot(x,y)\n",
        "print(f\"z.shape: {z.shape} çünkü z.ndim: {z.ndim} -> skaler bir sayı\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0y7pZ_v2ta2",
        "outputId": "0973edcb-ae6f-4483-97b3-10523ecd4769"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z.shape: () çünkü z.ndim: 0 -> skaler bir sayı\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_vector_dot(x,y):\n",
        "  assert len(x.shape) == 1\n",
        "  assert len(y.shape) == 1\n",
        "  assert x.shape[0] == y.shape[0]\n",
        "  z = 0.\n",
        "  for i in range(x.shape[0]):\n",
        "    z += x[i] * y[i]\n",
        "  return z\n",
        "\n",
        "print(f\"{naive_vector_dot(x,y)} vs {np.dot(x,y)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq-KIrPy24O-",
        "outputId": "fc35c82e-3d63-43ff-aaaf-b069c9ba9187"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.383818047803189 vs 9.383818047803189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([3,2,1])\n",
        "b = np.array([4,5,6])\n",
        "c = np.array([[7,2,5], [4, 1, 6]])\n",
        "print(np.dot(a,b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko143ARB33qn",
        "outputId": "2e4c21de-67bd-4310-c254-8f1289ac7167"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape, b.shape, c.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0Ie8evY4vNq",
        "outputId": "b78cdff2-b55c-48ce-fea9-9f405bd4170b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3,), (3,), (2, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_matrix_vector_dot(x,y):\n",
        "  assert len(x.shape) == 2\n",
        "  assert len(y.shape) == 1\n",
        "  assert x.shape[1] == y.shape[0]\n",
        "  z = np.zeros(x.shape[0])\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      z[i] += x[i, j] * y[j]\n",
        "  return z\n",
        "\n",
        "naive_matrix_vector_dot(c,a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Sd8GtZ6gXw",
        "outputId": "a6f9f592-5f57-4518-95e7-6975d533d410"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30., 20.])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_matrix_vector_dot(x,y):\n",
        "  z = np.zeros(x.shape[0])\n",
        "  for i in range(x.shape[0]):\n",
        "    z[i] = naive_vector_dot(x[i,:], y)\n",
        "  return z\n",
        "\n",
        "naive_matrix_vector_dot(c,a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp27FApe7OOM",
        "outputId": "f1aa854b-04b4-48f4-9b18-a5a180540748"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30., 20.])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = np.array([[7,2], [4, 1]])\n",
        "d = np.array([[3,1,6], [7, 4, 9]])\n",
        "\n",
        "def naive_matrix_dot(x,y):\n",
        "  assert x.shape[0] == 2\n",
        "  assert y.shape[0] == 2\n",
        "  assert x.shape[1] == y.shape[0]\n",
        "  z = np.zeros((x.shape[0],  y.shape[1]))\n",
        "\n",
        "  for i in range(x.shape[0],):\n",
        "    for j in range(y.shape[1]):\n",
        "      row_x = x[i, : ]\n",
        "      column_y = y[:, j]\n",
        "      z[i, j] = naive_vector_dot(row_x, column_y)\n",
        "  return z\n",
        "\n",
        "\n",
        "naive_matrix_dot(c,d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpJijbHK9EyX",
        "outputId": "eb35be58-681d-460e-d651-df8f4e418d01"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[35., 15., 60.],\n",
              "       [19.,  8., 33.]])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aw12R_xb9gZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MATRİSLERDE İŞLEMLERİ DAHA İYİ ANLAMAK İÇİN EGZERSİZLER\n",
        "Matrislerde işlem yapmaya daha da aşina olabilmek için uygulamalı lineer cebir kitabındaki örnekleri çözüyorum"
      ],
      "metadata": {
        "id": "wR5f3XYdCJp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matrislerde toplama\n",
        "\n",
        "A = np.array([[1,-2,3],\n",
        "              [2,-1,4]\n",
        "              ])\n",
        "B = np.array([[0,2,1],\n",
        "              [1,3,-4]\n",
        "              ])\n",
        "\n",
        "print(f\"A: {A.shape} vs B: {B.shape}\")\n",
        "\n",
        "print(f\"{A + B}\")\n",
        "\n",
        "z = np.zeros((A.shape[0], A.shape[1]))\n",
        "for i in range(A.shape[0]):\n",
        "  for j in range(A.shape[1]):\n",
        "    z[i, j] = A[i, j] + B[i, j]\n",
        "print(f\"{z}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdIwblh8CStH",
        "outputId": "948ce321-db1a-4d0a-92ab-13416bbf87e1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: (2, 3) vs B: (2, 3)\n",
            "[[1 0 4]\n",
            " [3 2 0]]\n",
            "[[1. 0. 4.]\n",
            " [3. 2. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# skalar çarpım\n",
        "A = np.array(-2)\n",
        "B = np.array([[4,-2,-3],\n",
        "              [7,-3,2]\n",
        "              ])\n",
        "\n",
        "z = np.zeros((B.shape[0], B.shape[1]))\n",
        "for i in range(B.shape[0]):\n",
        "  for j in range(B.shape[1]):\n",
        "    z[i,j] += A * B[i,j]\n",
        "print(z)\n",
        "\n",
        "\n",
        "print(A*B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mJBG3bfCr-0",
        "outputId": "de995ff8-ecf1-4635-ef35-4012dfdbf8ec"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ -8.   4.   6.]\n",
            " [-14.   6.  -4.]]\n",
            "[[ -8   4   6]\n",
            " [-14   6  -4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# matris farkı\n",
        "A = np.array([[2,3,-5],\n",
        "              [4,2,1]\n",
        "              ])\n",
        "\n",
        "B = np.array([[2,-1,3],\n",
        "              [3,5,-2]\n",
        "              ])\n",
        "\n",
        "A = A.copy()\n",
        "\n",
        "for i in range(A.shape[0]):\n",
        "  for j in range(A.shape[1]):\n",
        "      A[i, j] -= B[i, j]\n",
        "\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzPwEfeUEErK",
        "outputId": "d35cf4d6-5e90-4e10-e2bb-42c1cd66baf9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  4, -8],\n",
              "       [ 1, -3,  3]])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([\n",
        "    [0, -3, 5],\n",
        "    [2,3,4],\n",
        "    [1,-2,-3]])\n",
        "\n",
        "B = np.array([\n",
        "    [5, 2, 3],\n",
        "    [6,2,3],\n",
        "    [-1,-2,3]\n",
        "])\n",
        "\n",
        "z = np.dot(3,A) - np.dot(0.5, B)\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSg-wR-8Gc6k",
        "outputId": "9667d17f-4e62-456f-9ac7-4436c08e1208"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ -2.5, -10. ,  13.5],\n",
              "       [  3. ,   8. ,  10.5],\n",
              "       [  3.5,  -5. , -10.5]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X1gU4q4HHdnM"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor reshaping\n",
        "satır ve sütunları tekrardan düzenliyoruz\n"
      ],
      "metadata": {
        "id": "qxQEdZ3CIjUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([\n",
        "    [0., 1.],\n",
        "    [2., 3.],\n",
        "    [4., 5.]\n",
        "])\n",
        "\n",
        "print(x.shape,end=\"\\n--------------\\n\")\n",
        "x = x.reshape((6,1))\n",
        "print(x)\n",
        "print(x.shape,end=\"\\n--------------\\n\")\n",
        "x = x.reshape((2,3))\n",
        "print(x)\n",
        "print(x.shape,end=\"\\n--------------\\n\")\n",
        "\n",
        "# burada\n",
        "# 0 2 4\n",
        "# 1 3 5\n",
        "\n",
        "# şeklinde transpoz almak yerine\n",
        "\n",
        "# 0 1 2\n",
        "# 3 4 5\n",
        "\n",
        "# geldi çünkü reshape öncelikle hepsini tek bir satıra alıyor \"flatten\" yapıyor\n",
        "# [0 1 2 3 4 5] oluyor ve sonrasında ayırıyor\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du9Dz7hLIoQr",
        "outputId": "69cb5d61-8c30-4807-bb76-8c628011d5e7"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2)\n",
            "--------------\n",
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [5.]]\n",
            "(6, 1)\n",
            "--------------\n",
            "[[0. 1. 2.]\n",
            " [3. 4. 5.]]\n",
            "(2, 3)\n",
            "--------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.zeros((300, 20))\n",
        "x = np.transpose(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tlkn2KGI4I8",
        "outputId": "4adc28a9-b07e-4624-f8f1-d59cc00b4b45"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([\n",
        "    [0., 1.],\n",
        "    [2., 3.],\n",
        "    [4., 5.]\n",
        "])\n",
        "\n",
        "print(x,end=\"\\n------------\\n\")\n",
        "\n",
        "print(np.transpose(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY4_DxASK7Zm",
        "outputId": "9c732edf-5ee0-47a9-9d17-f322091de2e7"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1.]\n",
            " [2. 3.]\n",
            " [4. 5.]]\n",
            "------------\n",
            "[[0. 2. 4.]\n",
            " [1. 3. 5.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTdjjicfLDoc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}