{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Convnets'e giriş"
      ],
      "metadata": {
        "id": "ShcT-d_c7oMj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLnk23L-6me_"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(28,28,1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# input olarak convnet shape tensörü alır (image_height, image_width, image_channels).\n",
        "# bizim bu örnekte (28, 28, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "ITwbDXdT8MmM",
        "outputId": "ec376cc0-4e4d-477a-fb16-9c7ddbba7c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m11,530\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">11,530</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,202\u001b[0m (407.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,202</span> (407.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,202\u001b[0m (407.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,202</span> (407.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eğitim\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG8kcLVp8Njm",
        "outputId": "0a21d690-8964-4b9a-ef15-0ee4a69d1d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 34ms/step - accuracy: 0.8814 - loss: 0.3743\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 31ms/step - accuracy: 0.9856 - loss: 0.0470\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.9901 - loss: 0.0324\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.9929 - loss: 0.0245\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 32ms/step - accuracy: 0.9949 - loss: 0.0168\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x781c63619a10>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating the convnet\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy : {test_acc:.3f}\")\n",
        "# ilk chapterlarda yaptığımızda %97.8 civarlarında bir başarı vardı fakat şu anda %99.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QJspnEQ9UDW",
        "outputId": "fd3ab3c2-db03-4236-d51c-6a5c80880825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0330\n",
            "Test accuracy : 0.992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Konvolüsyon operasyonu\n",
        "Önceki densely connected layer yönteminde piksel piksel bakılarak bir pattern aranıyordu. Fakat Konvolüsyon ile yerel pattern'ler bulunuyor.\n",
        "\n",
        "2 önemli karakteristik özellik var:\n",
        "1. \"The patterns they learn are translation-invariant\". Yani sağ altta bir pattern gördüyse bu pattern'i nerede görürse görsün tanır. Sol üstte olması, ortada olması önemli değildir. Önceki yöntemimizde bu böyle değildi, aynı pattern farklı bir yerdeyse bunu tekrardan öğrenmesi gerekiyordu. Bu büyük bir özellik çünkü artık görsel verilerimizin tamamı yerden/konumdan bağımsız.\n",
        "\n",
        "2. \"They can learn spatial hiearchies of patterns\". İlk konvolüsyon katmanı koşeler gibi yerel pattern'leri öğreniyor, ikinci konvolüsyon katmanı daha büyük olan birinci katmanlarla oluşturulan daha büyük bir pattern'i öğreniyor. Böylelikle gittikçe öğrenilen pattern hiyerarşik olarak büyüyor. Görsel dünyadaki örüntüler hiyerarşik yapıdadır.\n",
        "\n",
        "\n",
        "\n",
        "Konvolüsyon operasyonu rank-3 bir tensör üzerinden gerçekleştirilir. `Feature maps`, iki uzamsal özellik: `height (yükseklik)`, `width (genişlik)` ve derinlik ekseni `channel axis`. RGB (Red-Green-Blue) bu derinlik ekseni 3'tür, çünkü görüntüde üç farklı renk vardır. Siyah-beyaz görsellerde bu değer 1'dir.\n",
        "\n",
        "Konvolüsyon işlemi, girişten küçük yama (patch) parçaları alır ve her yamaya aynı dönüşümü (filtreyi) uygular. Sonuç olarak `output feature map` oluşur. Bu çıktı yine üç boyutludur (yükseklik, genişlik, derinlik). Ancak artık bu çıktının derinliği RGB renkleri ile değil, filtreleri temsil eder.\n",
        "\n",
        "Her bir filtre, giriş görüntüdeki belli bir deseni yakalamaya çalışır.\n",
        "    - Bir filtre kenarları\n",
        "    - Başkası köşeleri\n",
        "    - Başkası yüz gibi daha karmaşık örüntüleri...\n",
        "\n",
        "\n",
        "## MNIST örneğinde\n",
        "- Giriş tensörü (28,28,1)\n",
        "- ilk konvolüsyon katmanı, 32 filtre uygular\n",
        "- her filtre, girişin üzerine kayarak (26,26) boyutunda bir çıktı üretir\n",
        "- Böylece çıktı tensörü (26, 26, 32) olur.\n",
        "\n",
        "\n",
        "## Feature map\n",
        "Derinlik eksenindeki her bir boyut, bir özellik (feature ya da filtre) temsil eder.\n",
        "Yani `output[:, :, n] şeklinde aldığın her 2D kısım, o filtrenin giriş üzerinde verdiği yanıtı gösterir.\n",
        "\n",
        "`depth` boyutundaki her bir kanal = bir filtre = bir özellik haritası\n",
        "\n",
        "## Konvolüsyonun 2 temel parametresi\n",
        "1. Patch boyutu\n",
        "    - Girişten alınan pencerenin boyutu (3x3 ya da 5x5)\n",
        "2. Çıktı derinliği (output depth)\n",
        "    - kaç farklı filtre uygulanacağını belirler (başlangıçta 32, sonra 64 filtre olabilir)\n",
        "\n",
        "`Conv2D(output_depth, (window_height, window_width))`\n",
        "\n",
        "##Konvolüsyon Nasıl Çalışır?\n",
        "\n",
        "- (3, 3) veya (5, 5) boyutunda pencere (window) alır.\n",
        "\n",
        "- Bu pencere, giriş tensörünün üzerinde kayarak her lokasyonda durur.\n",
        "\n",
        "- Her lokasyonda bir 3D yama çıkarılır:\n",
        "\n",
        "- `(window_height, window_width, input_depth)` şeklinde\n",
        "\n",
        "- Bu yama, öğrenilmiş bir ağırlık matrisi (kernel) ile çarpılır.\n",
        "\n",
        "- Bu kernel, bütün yamalarda aynı kalır.\n",
        "\n",
        "- Çıktı: `output_depth` boyutlu bir vektör\n",
        "\n",
        "- Tüm bu çıktılar, 3D bir output feature map'e birleştirilir:\n",
        "\n",
        "- `(height, width, output_depth)`"
      ],
      "metadata": {
        "id": "KrPwrOsY-qt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Çıktı Boyutu Neden Girişten Farklı Olabilir?\n",
        "\n",
        "1. Sınır Etkileri (Border Effects):\n",
        "\n",
        "  Görüntünün kenarında pencere tam oturamayabilir.\n",
        "\n",
        "  Çözüm: padding=\"same\" gibi yastıklama yapılır.\n",
        "\n",
        "2. Stride (Adım Boyu):\n",
        "\n",
        "  Pencerenin kaç pikselde bir kayacağını belirler.\n",
        "\n",
        "  Örn: stride=2 → pencere her seferinde 2 piksel kayar → çıktı küçülür.\n",
        "\n",
        "\n",
        "## Border effects ve padding'i anlayalım\n",
        "Padding Neden Gerekli?\n",
        "\n",
        "Eğer çıktının da girişle aynı boyutta olmasını istiyorsan, bu kaybı önlemelisin.\n",
        "Bunun için:\n",
        "\n",
        "- Girişin etrafına sıfırlarla dolu satır/sütunlar eklersin (padding).\n",
        "\n",
        "- Böylece filtre her yerde rahatça dolaşabilir.\n",
        "\n",
        "`Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\")`\n",
        "\n",
        "- \"valid\" → padding yok → çıktı daha küçük\n",
        "\n",
        "- \"same\" → gerekli padding eklenir → çıktı girişle aynı boyutta\n",
        "\n",
        "$$\n",
        "\\text{Output size} = \\frac{\\text{Input size} - \\text{Filter size}}{\\text{Stride}} + 1\n",
        "$$\n",
        "\n",
        "# Konvolüsyon adımlarını (strides) anlayalım\n",
        "Strides dediğimiz kavram filtrenin kaç piksel ilerleyeceğini belirtir. Varsayılan olarak 1'dir ve birer piksel olarak fırçamızı ilerletir.\n",
        "\n",
        "Filtre daha az konumda uygulandığı için çıkış boyutu düşer.\n",
        "\n",
        "Sınıflandırma modellerinde, feature maps'i azaltmak için strides yerine daha çok `max-pooling` kullanırız.\n",
        "\n",
        "## Max-pooling operasyonu\n",
        "\n",
        "- Max pooling, giriş olarak bir özellik haritası (feature map) alır. Bu, genellikle konvolüsyon katmanından çıkan veridir.\n",
        "\n",
        "- Max pooling işlemi, bu özellik haritasını küçük pencerelere (window) böler.\n",
        "- Genellikle bu pencere boyutu 2×2’dir.\n",
        "\n",
        "- Her pencerede, penceredeki en büyük (maksimum) değer seçilir.\n",
        "\n",
        "- Bu seçilen maksimum değer, o pencere için çıktı olur."
      ],
      "metadata": {
        "id": "QsKJsXvIEy7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max-pooling katmanı olmadan hatalı bir şekilde convnet oluşturmak\n",
        "\n",
        "inputs = keras.Input(shape=(28,28,1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "pUGGJwuW9-y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_max_pool.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "DNsR_xLpQ8e4",
        "outputId": "3e481e7e-8386-4aee-fa1c-f2aa71183c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61952\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │       \u001b[38;5;34m619,530\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61952</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">619,530</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m712,202\u001b[0m (2.72 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">712,202</span> (2.72 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m712,202\u001b[0m (2.72 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">712,202</span> (2.72 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Burada iki problem var:\n",
        "1. **Model çok küçük alanlara bakıyor:**  \n",
        "Son katmandaki filtreler girişin sadece küçük bir parçasını (7×7 piksel) görebiliyor. Bu yüzden daha büyük, karmaşık desenleri tam öğrenemiyor.\n",
        "\n",
        "2. **Parametre sayısı çok fazla:**  \n",
        "Son konvolüsyon katmanından çıkan 22×22×128 boyutundaki veri flatten edilince, Dense katmanda yarım milyondan fazla parametre oluyor. Bu da aşırı öğrenmeye yol açıyor.\n",
        "\n",
        "\n",
        "**Eğer max-pooling olsaydı:**\n",
        "\n",
        "- Max-pooling feature map’in boyutunu örneğin 2 kat küçültür (22×22 → 11×11 gibi).\n",
        "- Böylece son katmandaki çıktı daha küçük olur, parametre sayısı önemli ölçüde azalır.\n",
        "- Ayrıca model, farklı seviyelerdeki özellikleri daha geniş alanlarda görebilir; yani mekânsal hiyerarşi öğrenmesi kolaylaşır.\n",
        "- Sonuç olarak model daha verimli öğrenir, aşırı öğrenme riski düşer.\n"
      ],
      "metadata": {
        "id": "ygRNbtxuR2U-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a convnet from scratch on a small dataset:\n",
        "1. küçük bir modeli sıfırdan eğit.\n",
        "2. daha önceden eğitilmiş bir model ile feature extraction\n",
        "3. daha önceden eğitilmiş bir model ile fine-tuning"
      ],
      "metadata": {
        "id": "qNlk3uJoGxeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Veriyi indirme\n",
        "kaggle'dan API İle indiriyoruz."
      ],
      "metadata": {
        "id": "rUujzlvRHEj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# authenticate olmak için dosya oluşturuyoruz.\n",
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "cd8ucDY2HQ2C",
        "outputId": "0eab35f4-11b4-466c-dd91-50ae7bfe19dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a0900102-69c7-43aa-b3ed-c22e21665305\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a0900102-69c7-43aa-b3ed-c22e21665305\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle (1).json': b'{\"username\":\"gokhangokcen\",\"key\":\"3aaa1287bd1ca904763a585ff581e30f\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLPDtGVgHssR",
        "outputId": "309f1cbf-74f9-4733-a8ce-6d70ec95bc18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c dogs-vs-cats\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6eBokMFH_L-",
        "outputId": "4869b80b-9783-47ef-f3e0-9549d12f3289"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/competitions/data/download-all/dogs-vs-cats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "artık veri seti kullanılamaz olduğu için microsoft'tan indirip buraya yükledik."
      ],
      "metadata": {
        "id": "h6CDMGtCM1Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "qW7LB900gEjm",
        "outputId": "3cd43ba2-f826-4685-95c6-9f55caa567db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'files' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-4221232239.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n2y9tdVo7PJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "import os, shutil, pathlib\n",
        "\n",
        "# Orijinal verinin bulunduğu klasör (PetImages/ klasöründen alınan dosyaların taşındığı yer)\n",
        "original_dir = pathlib.Path(\"train\")  # Eğer \"train\" klasörü değilse, burayı değiştir\n",
        "\n",
        "# Yeni küçük veri seti dizini\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "# Verileri alt klasörlere ayırmak için fonksiyon\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "        \n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        \n",
        "        for fname in fnames:\n",
        "            src = original_dir / fname\n",
        "            dst = dir / fname\n",
        "            if src.exists():  # Dosya gerçekten var mı kontrol et (hataları önler)\n",
        "                shutil.copyfile(src, dst)\n",
        "\n",
        "# Alt kümeleri oluştur\n",
        "make_subset(\"train\", start_index=0, end_index=1000)\n",
        "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
        "make_subset(\"test\", start_index=1500, end_index=2500)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "wO-iCfiBRgYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "# Burada \"PetImages\" klasörünü belirtiyoruz\n",
        "original_base_dir = pathlib.Path(\"PetImages\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"Cat\", \"Dog\"):\n",
        "        src_dir = original_base_dir / category\n",
        "        dst_dir = new_base_dir / subset_name / category.lower()  # \"cat\" ve \"dog\" olacak\n",
        "        os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "        # Sadece .jpg uzantılı ve bozuk olmayan dosyaları al\n",
        "        fnames = sorted([\n",
        "            fname for fname in os.listdir(src_dir)\n",
        "            if fname.lower().endswith(\".jpg\") and os.path.getsize(src_dir / fname) > 0\n",
        "        ])\n",
        "\n",
        "        selected_fnames = fnames[start_index:end_index]\n",
        "\n",
        "        for fname in selected_fnames:\n",
        "            src = src_dir / fname\n",
        "            dst = dst_dir / fname\n",
        "            try:\n",
        "                shutil.copyfile(src, dst)\n",
        "            except:\n",
        "                pass  # bazen bozuk dosya varsa, geç\n",
        "\n",
        "# Altsetleri oluştur\n",
        "make_subset(\"train\", 0, 1000)\n",
        "make_subset(\"validation\", 1000, 1500)\n",
        "make_subset(\"test\", 1500, 2500)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "snL5cdbYJmHv",
        "outputId": "15a9d22d-b60a-4188-c4b8-3a166c36267a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'PetImages/Cat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-227956618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Altsetleri oluştur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmake_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mmake_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mmake_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-8-227956618.py\u001b[0m in \u001b[0;36mmake_subset\u001b[0;34m(subset_name, start_index, end_index)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Sadece .jpg uzantılı ve bozuk olmayan dosyaları al\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         fnames = sorted([\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mfname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         ])\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PetImages/Cat'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "artık 2000 eğitim, 1000 validation, 2000 de test görselimiz var."
      ],
      "metadata": {
        "id": "V34ggenUNDYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeli oluşturmak\n",
        "Başlangıç inputu olarak 180 x 180 piksel seçiyoruz, sonrasında feature map'inin boyutunu da 77x7 seçiyoruz, Flatten katmanında önce.\n",
        "\n",
        "Binary classification yaptığımız için son katmanda sigmoid aktivasyon fonksiyonlu bir unit olacak.\n",
        "\n",
        "Farklı olarak, modeli Rescaling katmanı ile başlatacağız ve görselleri [0, 255] aralığından [0,1] aralığına getireceğiz."
      ],
      "metadata": {
        "id": "yu1QhEorNJSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = layers.Rescaling(1./255)(inputs) # [0,1] aralığına aldı\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x) # 180x180'dan birer piksel kırpıldı ve 178,178,32 oldu\n",
        "x = layers.MaxPooling2D(pool_size=2)(x) # yarıya düştü: 89,89,32\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x) # 87, 87, 64\n",
        "x = layers.MaxPooling2D(pool_size=2)(x) # 43, 43, 64\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x) # 41, 41, 128\n",
        "x = layers.MaxPooling2D(pool_size=2)(x) # 20, 20, 128\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x) # 18, 18, 256\n",
        "x = layers.MaxPooling2D(pool_size=2)(x) # 9, 9, 256\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x) # 7, 7, 256\n",
        "x = layers.Flatten()(x) # 12544\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # binary classification olduğu için 1\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "-Q80b0RUM777"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "J3XslPa3N8n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Model: \"model_2\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                     Output Shape           Param #\n",
        "=================================================================\n",
        "input_3 (InputLayer)             [(None, 180, 180, 3)]          0\n",
        "_________________________________________________________________\n",
        "rescaling (Rescaling)            (None, 180, 180, 3)            0\n",
        "_________________________________________________________________\n",
        "conv2d_6 (Conv2D)                (None, 178, 178, 32)         896\n",
        "_________________________________________________________________\n",
        "max_pooling2d_2 (MaxPooling2    (None, 89, 89, 32)              0\n",
        "_________________________________________________________________\n",
        "conv2d_7 (Conv2D)               (None, 87, 87, 64)          18496\n",
        "_________________________________________________________________\n",
        "max_pooling2d_3 (MaxPooling2    (None, 43, 43, 64)              0\n",
        "_________________________________________________________________\n",
        "conv2d_8 (Conv2D)               (None, 41, 41, 128)         73856\n",
        "_________________________________________________________________\n",
        "max_pooling2d_4 (MaxPooling2    (None, 20, 20, 128)             0\n",
        "_________________________________________________________________\n",
        "conv2d_9 (Conv2D)               (None, 18, 18, 256)        295168\n",
        "_________________________________________________________________\n",
        "max_pooling2d_5 (MaxPooling2    (None, 9, 9, 256)               0\n",
        "_________________________________________________________________\n",
        "conv2d_10 (Conv2D)              (None, 7, 7, 256)          590080\n",
        "_________________________________________________________________\n",
        "flatten_2 (Flatten)             (None, 12544)                   0\n",
        "_________________________________________________________________\n",
        "dense_2 (Dense)                 (None, 1)                   12545\n",
        "=================================================================\n",
        "Total params: 991,041\n",
        "Trainable params: 991,041\n",
        "Non-trainable params: 0\n",
        "_______________________________________________________________\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "t2wPzdpZQ7h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "ganEYdfZN-7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing\n",
        "Adımlar:\n",
        "1. Resim dosyalarını oku\n",
        "2. JPEG içeriklerini RGB piksel ızgaralarına dönüştür.\n",
        "3. Bunları floating-point tensörlere çevir\n",
        "4. Boyutlarını düzenle (resize) (180x180)\n",
        "5. Hepsini batchlere böl (32 images)\n",
        "\n",
        "Keras bu adımları otomatik olarak yapıyor image_dataset_from_directory()."
      ],
      "metadata": {
        "id": "JpoCrOHiRtmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image_dataset_from_directory ile resim okuma\n",
        "\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32\n",
        ")\n"
      ],
      "metadata": {
        "id": "T9DLUt1mSL4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Veri yükleme ve işlemede tf.data.Dataset\n",
        "\n",
        "`tf.data.Dataset`, modeli beslemenin standart yoludur.\n",
        "- For döngüsü ile kullanılabilir\n",
        "- fit fonksiyonuna doğrudan verilebilir.\n",
        "\n",
        "## NumPy'den dataset oluşturmak\n",
        "```\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "random_numbers = np.random.normal(size=(1000,16))\n",
        "dataset = tf.dataDataset.from_tensor_slices(random_numbers)\n",
        "```\n",
        "\n",
        "tek tek sample verir.\n",
        "```\n",
        "for i, element in enumerate(dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break\n",
        "\n",
        "# (16,)\n",
        "# (16,)\n",
        "# (16,)\n",
        "\n",
        "```\n",
        "\n",
        "## `.batch()` - veriyi gruplamak\n",
        "\n",
        "`batched_dataset = dataset.batch(32)`\n",
        "\n",
        "artık for döngüsü 32'lik bloklarla çalışır.\n",
        "\n",
        "```\n",
        "for i, element in enumerate(batched_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break\n",
        "\n",
        "\n",
        "# (32, 16)\n",
        "# (32, 16)\n",
        "# (32, 16)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Diğer fonksiyonlar\n",
        "- `.shuffle(buffer_size)` -> shuffle\n",
        "- `.batch(batch_size)` -> batchlere ayır\n",
        "- `.map(callable)` -> veriye dönüşüm uygular (normalization, augmentation)\n",
        "- `.prefetch(buffer_size)` -> sonraki batch'i hazırlayıp RAM veya GPU'ya yükler\n",
        "\n",
        "\n",
        "## `.map()` - Veri dönüştürme\n",
        "`reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))`\n",
        "\n",
        "(16,) -> (4,4)\n",
        "\n",
        "\n",
        "```\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "# (4, 4)\n",
        "# (4, 4)\n",
        "# (4, 4)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "dHOZbV7MSg2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset object'lerinin çıkışına bakalım. 180x180 RGB resimler (shape (32, 180, 180, 3)) ve integer labels (shape (32,0)). Her bir batch'te 32 örnek var."
      ],
      "metadata": {
        "id": "3JYE-Nv4acVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for data_batch, labels_batch in train_dataset:\n",
        "    print(\"data batch shape:\", data_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    break\n",
        "\n",
        "# data batch shape: (32, 180, 180, 3)\n",
        "# labels batch shape: (32, )"
      ],
      "metadata": {
        "id": "y4n10YsfaZ1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint( # her epoch'ta kaydet\n",
        "        filepath=\"convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\"\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "m10zwvt0a-cV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6mMTvX2ibvCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting oluyor. 10 epoch'tan sonra aradaki fark açılmaya başlıyor. train_acc %100'e yaklaşırken, val_acc %75lerde kalıyor"
      ],
      "metadata": {
        "id": "ooJ5fJzJbyyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test veri setinde deneyelim\n",
        "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "rMmKEhqAcB3e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "%69 doğruluk. Başlangıç için iyi. Fakat örnek sayımız çok az olduğu için (2000) overfit olmaya eğilimli. Overfitting'ten kaçmak için: droput, L2 regularization, early stopping kullanılabilir.\n",
        "\n",
        "\n",
        "Bunların haricinde kullanılabilecek bir diğer teknik de `Data augmentation`."
      ],
      "metadata": {
        "id": "eNA6TyoRcIcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data augmentation\n",
        "Overfitting örnek sayısının yetersizliğinden kaynaklanıyor. Data augmentation verilerimizi çeşitlendirmeyi sağlıyor. Aynı fotoğrafı iki kere vermiyoruz fakat bazı özellikleri değişmiş eğitim fotoğraflarını tekrar verebiliriz. Böylelikle daha iyi bir genelleme yapılması sağlanabilir.\n",
        "\n",
        "Keras'ta data augmentation katmanları eklenebilir.\n",
        "- Rescaling'ten önce verilir."
      ],
      "metadata": {
        "id": "RkO4YQoUc4VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"), # %50 ihtimalle yatay çevir\n",
        "        layers.RandomRotation(0.1), # +-%10  yaklaşık (36 derece döndür)\n",
        "        layers.RandomZoom(0.2), # +-%20 yakınlaştır/uzaklaştır\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "lbC5BzitcG7w"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(fig_size=(10, 10))\n",
        "\n",
        "for images, _ in train_dataset.take(1): # 1. örnek\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images)\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "FOFzupBDeSTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a new convnet that includes image augmentation and dropout\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "optimizer=\"rmsprop\",\n",
        "metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "ILG4-IHhes73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the regularized convnet\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\"\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=100,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "wOthZZDCexC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Artık validation %80-85 doğrulukta."
      ],
      "metadata": {
        "id": "fOLRnU64e4ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = keras.models.load_model(\n",
        "        \"convnet_from_scratch_with_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "6jxCPqque9E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test ise %83.5 doğrulukta."
      ],
      "metadata": {
        "id": "tQgtGUVwe_qc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5es6c0dSfIk9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}