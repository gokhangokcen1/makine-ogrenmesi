{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST VERİ SETİ"
      ],
      "metadata": {
        "id": "eERRKr8oOn0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "AZYqqjv1MHv6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMOR_XZZMWtG",
        "outputId": "39c97c76-cbeb-4cfa-d12d-7ea9c346e19f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwk-GoxjMZ50",
        "outputId": "d78b1c9a-4d01-4eb6-988e-bde918272d7e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwiZhGluMdJF",
        "outputId": "b48b654f-7268-4f36-f522-c53fd37feb46"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PivghQFqMeq9",
        "outputId": "73c989ac-a68a-443f-9694-d7b9c9957a50"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S7n3TZDMhHk",
        "outputId": "f8f21f2a-edc4-4aad-b5d3-dbff63ed6acd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x03hfzD5MjQT",
        "outputId": "ea83ab21-38b2-4849-98a8-c3be3ca0a23b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "CYLo0uDPMkhg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "sljqJdbHM1Ja"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ],
      "metadata": {
        "id": "GXITnj5ZNHbk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm4OQX_ZM-80",
        "outputId": "87894e55-caf9-4b62-fb41-6e9c87691408"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8733 - loss: 0.4424\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.9670 - loss: 0.1172\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.9791 - loss: 0.0706\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9846 - loss: 0.0504\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.9892 - loss: 0.0363\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a9cb34c3250>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_digits = test_images[0:10]\n",
        "predictions = model.predict(test_digits)\n",
        "predictions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVrhizThNcMx",
        "outputId": "06ee431a-8171-497a-8d5d-28b1ae323f17"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.5594704e-07, 5.5816629e-10, 4.4205322e-06, 5.9928185e-05,\n",
              "       7.2840614e-12, 3.9741916e-08, 1.0141539e-12, 9.9993116e-01,\n",
              "       3.5975809e-08, 4.1243520e-06], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0].argmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fj8c5D4OLqr",
        "outputId": "d2f57d02-2191-4f66-8a35-6cb703ef365b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(7)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0][7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c02azJmEORds",
        "outputId": "49a1f6a4-c01f-41ad-e75f-b073696466a9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(0.99993116)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gni4FzIjOSo-",
        "outputId": "e2a4579b-a33e-4f20-ddd9-25c3e0d174f7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.uint8(7)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"test_acc: {test_acc}\")\n",
        "print(f\"test_loss: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lj43yY9OUil",
        "outputId": "3812f414-ddab-4082-f90d-a3ae95ebf318"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9784 - loss: 0.0769\n",
            "test_acc: 0.9815000295639038\n",
            "test_loss: 0.06508830934762955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SİNİR AĞLARI İÇİN VERİ GÖSTERİMİ"
      ],
      "metadata": {
        "id": "SPQ7_739Ol41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scalar -> tek boyutlu vektör, rank(0)\n",
        "import numpy as np\n",
        "x = np.array(12)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5_KOw3COwja",
        "outputId": "29c42e35-e35f-4724-9c12-2745af628eda"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(12)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D5IQaVQPGfY",
        "outputId": "f336f9f7-896b-4cdf-90d5-3723962b0261"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vektör rank 1 tensors\n",
        "x = np.array([12, 3, 6, 14, 7])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTbj3mwEPHV9",
        "outputId": "8f045e90-d53d-43d2-88de-e5818335d3c3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12,  3,  6, 14,  7])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.ndim # 5 adet verisi ver. 5D vector ama 1D tensor,\n",
        "#her bir [] tensör diyebiliriz."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qtiG3qWP2zF",
        "outputId": "ab2940b1-ee41-4038-9626-7b9ee97b03ba"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# matris -> 2D tensors\n",
        "x = np.array([[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35 ,1],\n",
        "              [7, 80, 4, 36, 2]])\n",
        "x.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2zc04cEP38a",
        "outputId": "00223e77-0e17-49a4-e644-aaf2b7f1ac0d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rank 3 -> 3D tensors\n",
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35 ,1],\n",
        "              [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35 ,1],\n",
        "              [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35 ,1],\n",
        "              [7, 80, 4, 36, 2]]])\n",
        "\n",
        "x.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wf2pAgwQn0j",
        "outputId": "afffcb3e-9162-4bdc-93da-0ec9fcd06479"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "digit = train_images[4]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "p2xmfTwJQwo3",
        "outputId": "75c05b6a-3652-49f9-c7ff-dd3313d2c7d6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mM__eCCSl6O",
        "outputId": "d4de18f8-d493-40ae-8018-810b2eea8adf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.uint8(9)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_slice = train_images[10:100]\n",
        "my_slice.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b40zceP5S1Ge",
        "outputId": "2af35596-9513-4269-84d7-ce2d014bd04c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_slice = train_images[10:100, : , : ]\n",
        "my_slice.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxF-fTKAS_N0",
        "outputId": "ec063855-1bcd-45bd-934c-c0e110839f4f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_slice = train_images[10:100, 0:28, 0:28]\n",
        "my_slice.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "387FjV-KTH3f",
        "outputId": "008ee89d-e647-4c05-909a-3bc82cdff24e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_slice = train_images[:, 14:, 14:]\n",
        "my_slice.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgHMQC8OTOA_",
        "outputId": "ab27941a-a2f6-4a65-fd78-af252aa9c19a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 14, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_digit = train_images[4, 7:-7, :-7]\n",
        "plt.imshow(train_digit)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "24Q9mDxITYr-",
        "outputId": "761ee44a-2261-4dd5-a97e-0396311121bb"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LHv5p8xbTmwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BATCH"
      ],
      "metadata": {
        "id": "vW6flsS7UCaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batchler bir gruplar, model genel olarak datasetini tek seferde işlemez, böler"
      ],
      "metadata": {
        "id": "CxBTiDLNUE9i"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = train_images[:128]\n",
        "batch = train_images[128:255]"
      ],
      "metadata": {
        "id": "hUozR-z8UN8H"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5j0ndt8UUy8",
        "outputId": "aad9db9a-df11-4905-aa14-4d5ce453c4b9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(127, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n. batch\n",
        "n = 3\n",
        "batch = train_images[128 * n: 128 * (n+1)]\n",
        "batch.shape\n",
        "\n",
        "'''\n",
        "veri setini 128 128 olarak bölüyoruz\n",
        "kaçıncı 128'lik bölümü aradığımızı bu şekilde buluyoruz\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EbQlfJH3UXur",
        "outputId": "b3b54579-9188-4e0e-f865-7f65f6d758e3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nveri setini 128 128 olarak bölüyoruz\\nkaçıncı 128'lik bölümü aradığımızı bu şekilde buluyoruz\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3BVHGiFEUm9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# elle relu yazma"
      ],
      "metadata": {
        "id": "-8wNebSQcbC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([\n",
        "    [-1, 2, -3],\n",
        "    [4, -5, 6]\n",
        "])"
      ],
      "metadata": {
        "id": "fCxfJmeeccTq"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_relu(x):\n",
        "  assert len(x.shape) == 2\n",
        "  x = x.copy()\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i, j] = max(x[i,j], 0)\n",
        "  return x\n",
        "\n",
        "naive_relu(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8NAW2BtciHK",
        "outputId": "c1282be7-8f41-4187-d397-28b3c4efb222"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 2, 0],\n",
              "       [4, 0, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.array([\n",
        "    [2, -4, 7],\n",
        "    [-2, 9, -1]\n",
        "])\n",
        "\n",
        "def naive_add(x, y):\n",
        "  assert len(x.shape) == 2\n",
        "  assert x.shape == y.shape\n",
        "  x = x.copy()\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i ,j] += y[i, j]\n",
        "\n",
        "  return x\n",
        "\n",
        "print(X)\n",
        "print(Y)\n",
        "naive_add(X,Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAf24Sw_c3Sx",
        "outputId": "6eb6ba04-e457-444e-9c84-884947ca61d3"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1  2 -3]\n",
            " [ 4 -5  6]]\n",
            "[[ 2 -4  7]\n",
            " [-2  9 -1]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1, -2,  4],\n",
              "       [ 2,  4,  5]])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kendi yazdığımız ile zaman farkı\n",
        "\n",
        "import time\n",
        "x = np.random.random((20,100))\n",
        "y = np.random.random((20,100))\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "  z = x + y\n",
        "  z = np.maximum(z, 0.)\n",
        "  print(\"Took: {0:.2f} s\".format(time.time() - t0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EAgleWve8ig",
        "outputId": "06886241-c20e-46d2-8265-bfab9e4db7da"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.00 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.01 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n",
            "Took: 0.02 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "  z = naive_add(x,y)\n",
        "  z = naive_relu(z)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8fV5oQKfhBX",
        "outputId": "887c82e7-d328-412f-9a57-cf04548d6d27"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 1.93 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Broadcast\n",
        "\n",
        "farklı shape'e ve boyuta sahip tensörlerde işlem yapmak. 2D bir tensor ile 1D vektör toplanacaksa, vektör tüm 2D elemanlarına uygulanır (broadcast edilir).\n"
      ],
      "metadata": {
        "id": "Chrigm1oggYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.random.random((32, 10)) # 32 x 10 matris\n",
        "y = np.random.random((10,)) # 10 x 1 vector\n",
        "print(f\"x_dim : {x.ndim} and x_shape : {x.shape}\\ny_dim : {y.ndim} and y_shape : {y.shape}\")\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw6T3bZ2f1WZ",
        "outputId": "8571b0e4-c4c2-4721-929d-d3744729fd8a"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_dim : 2 and x_shape : (32, 10)\n",
            "y_dim : 1 and y_shape : (10,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "       0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.expand_dims(y, axis = 0)\n",
        "print(f\"y_dim : {y.ndim} and y_shape : {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34565VWTg0Lo",
        "outputId": "971269da-e4d7-468d-ed23-fe1d65ca6e81"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_dim : 2 and y_shape : (1, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDghtQyfhF7n",
        "outputId": "ec78349b-af69-481f-b67f-fd043d433d4e"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577]])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.concatenate([y] * 32, axis=0)\n",
        "print(f\"y_dim : {y.ndim} and y_shape : {y.shape}\")\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I_9VYr2g0Z6",
        "outputId": "92dd46c8-aed4-4ce5-9888-4182b3550a1b"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_dim : 2 and y_shape : (32, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577],\n",
              "       [0.74454922, 0.33619262, 0.21255552, 0.68579545, 0.15154048,\n",
              "        0.89750789, 0.31898802, 0.55759564, 0.15320677, 0.35310577]])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_add_matrix_and_vector(x,y):\n",
        "  assert len(x.shape) == 2\n",
        "  assert len(y.shape) == 1\n",
        "  x = x.copy()\n",
        "\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i, j] += y[i, j]\n",
        "  return x"
      ],
      "metadata": {
        "id": "kZhjNgDSiRU3"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# peki hangi değişken broadcast yapılmasına daha uygun\n",
        "\n",
        "x = np.random.random((64, 3, 32, 10))\n",
        "y = np.random.random((32,10))\n",
        "z = np.maximum(x,y)\n",
        "print(z.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1EUFtrh2KyF",
        "outputId": "1bc7f18f-9531-42db-b593-a8c00f6fd523"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 3, 32, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIyz1y0E2j4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tensor çarpımı -> nokta çarpımı\n"
      ],
      "metadata": {
        "id": "Eck87hVn2rF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.random((32,))\n",
        "y = np.random.random((32,))\n",
        "z = np.dot(x,y)\n",
        "print(f\"z.shape: {z.shape} çünkü z.ndim: {z.ndim} -> skaler bir sayı\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0y7pZ_v2ta2",
        "outputId": "11b20c82-0e74-487d-8bd3-212efd6760f8"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z.shape: () çünkü z.ndim: 0 -> skaler bir sayı\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_vector_dot(x,y):\n",
        "  assert len(x.shape) == 1\n",
        "  assert len(y.shape) == 1\n",
        "  assert x.shape[0] == y.shape[0]\n",
        "  z = 0.\n",
        "  for i in range(x.shape[0]):\n",
        "    z += x[i] * y[i]\n",
        "  return z\n",
        "\n",
        "print(f\"{naive_vector_dot(x,y)} vs {np.dot(x,y)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq-KIrPy24O-",
        "outputId": "4e95b3ae-0b58-49cd-ccb1-c3004e0aacf5"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.525104966109826 vs 9.525104966109827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([3,2,1])\n",
        "b = np.array([4,5,6])\n",
        "c = np.array([[7,2,5], [4, 1, 6]])\n",
        "print(np.dot(a,b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko143ARB33qn",
        "outputId": "f8327b85-6938-4a34-d76d-e3244396f4fc"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape, b.shape, c.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0Ie8evY4vNq",
        "outputId": "33f48cc4-8340-456a-b217-6a93d494ef2d"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3,), (3,), (2, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_matrix_vector_dot(x,y):\n",
        "  assert len(x.shape) == 2\n",
        "  assert len(y.shape) == 1\n",
        "  assert x.shape[1] == y.shape[0]\n",
        "  z = np.zeros(x.shape[0])\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      z[i] += x[i, j] * y[j]\n",
        "  return z\n",
        "\n",
        "naive_matrix_vector_dot(c,a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Sd8GtZ6gXw",
        "outputId": "3ae499b2-6e48-4843-af31-2a2a119aebc4"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30., 20.])"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_matrix_vector_dot(x,y):\n",
        "  z = np.zeros(x.shape[0])\n",
        "  for i in range(x.shape[0]):\n",
        "    z[i] = naive_vector_dot(x[i,:], y)\n",
        "  return z\n",
        "\n",
        "naive_matrix_vector_dot(c,a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp27FApe7OOM",
        "outputId": "380704c6-7016-4a56-e2cc-29dfba8dbeab"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30., 20.])"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = np.array([[7,2], [4, 1]])\n",
        "d = np.array([[3,1,6], [7, 4, 9]])\n",
        "\n",
        "def naive_matrix_dot(x,y):\n",
        "  assert x.shape[0] == 2\n",
        "  assert y.shape[0] == 2\n",
        "  assert x.shape[1] == y.shape[0]\n",
        "  z = np.zeros((x.shape[0],  y.shape[1]))\n",
        "\n",
        "  for i in range(x.shape[0],):\n",
        "    for j in range(y.shape[1]):\n",
        "      row_x = x[i, : ]\n",
        "      column_y = y[:, j]\n",
        "      z[i, j] = naive_vector_dot(row_x, column_y)\n",
        "  return z\n",
        "\n",
        "\n",
        "naive_matrix_dot(c,d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpJijbHK9EyX",
        "outputId": "806c13d3-39bb-4ff9-8d8e-3b08317a5b6c"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[35., 15., 60.],\n",
              "       [19.,  8., 33.]])"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aw12R_xb9gZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MATRİSLERDE İŞLEMLERİ DAHA İYİ ANLAMAK İÇİN EGZERSİZLER\n",
        "Matrislerde işlem yapmaya daha da aşina olabilmek için uygulamalı lineer cebir kitabındaki örnekleri çözüyorum"
      ],
      "metadata": {
        "id": "wR5f3XYdCJp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matrislerde toplama\n",
        "\n",
        "A = np.array([[1,-2,3],\n",
        "              [2,-1,4]\n",
        "              ])\n",
        "B = np.array([[0,2,1],\n",
        "              [1,3,-4]\n",
        "              ])\n",
        "\n",
        "print(f\"A: {A.shape} vs B: {B.shape}\")\n",
        "\n",
        "print(f\"{A + B}\")\n",
        "\n",
        "z = np.zeros((A.shape[0], A.shape[1]))\n",
        "for i in range(A.shape[0]):\n",
        "  for j in range(A.shape[1]):\n",
        "    z[i, j] = A[i, j] + B[i, j]\n",
        "print(f\"{z}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdIwblh8CStH",
        "outputId": "01e39083-af4d-4eb0-8dea-de82003f329c"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: (2, 3) vs B: (2, 3)\n",
            "[[1 0 4]\n",
            " [3 2 0]]\n",
            "[[1. 0. 4.]\n",
            " [3. 2. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# skalar çarpım\n",
        "A = np.array(-2)\n",
        "B = np.array([[4,-2,-3],\n",
        "              [7,-3,2]\n",
        "              ])\n",
        "\n",
        "z = np.zeros((B.shape[0], B.shape[1]))\n",
        "for i in range(B.shape[0]):\n",
        "  for j in range(B.shape[1]):\n",
        "    z[i,j] += A * B[i,j]\n",
        "print(z)\n",
        "\n",
        "\n",
        "print(A*B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mJBG3bfCr-0",
        "outputId": "eda7e355-5b92-4e14-c574-84c67c07d9f5"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ -8.   4.   6.]\n",
            " [-14.   6.  -4.]]\n",
            "[[ -8   4   6]\n",
            " [-14   6  -4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# matris farkı\n",
        "A = np.array([[2,3,-5],\n",
        "              [4,2,1]\n",
        "              ])\n",
        "\n",
        "B = np.array([[2,-1,3],\n",
        "              [3,5,-2]\n",
        "              ])\n",
        "\n",
        "A = A.copy()\n",
        "\n",
        "for i in range(A.shape[0]):\n",
        "  for j in range(A.shape[1]):\n",
        "      A[i, j] -= B[i, j]\n",
        "\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzPwEfeUEErK",
        "outputId": "23e88592-8699-45c4-c0ae-d5259b62d41f"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  4, -8],\n",
              "       [ 1, -3,  3]])"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([\n",
        "    [0, -3, 5],\n",
        "    [2,3,4],\n",
        "    [1,-2,-3]])\n",
        "\n",
        "B = np.array([\n",
        "    [5, 2, 3],\n",
        "    [6,2,3],\n",
        "    [-1,-2,3]\n",
        "])\n",
        "\n",
        "z = np.dot(3,A) - np.dot(0.5, B)\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSg-wR-8Gc6k",
        "outputId": "44d9cb26-6d19-472b-ac09-39c719c19409"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ -2.5, -10. ,  13.5],\n",
              "       [  3. ,   8. ,  10.5],\n",
              "       [  3.5,  -5. , -10.5]])"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X1gU4q4HHdnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor reshaping\n",
        "satır ve sütunları tekrardan düzenliyoruz\n"
      ],
      "metadata": {
        "id": "qxQEdZ3CIjUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([\n",
        "    [0., 1.],\n",
        "    [2., 3.],\n",
        "    [4., 5.]\n",
        "])\n",
        "\n",
        "print(x.shape,end=\"\\n--------------\\n\")\n",
        "x = x.reshape((6,1))\n",
        "print(x)\n",
        "print(x.shape,end=\"\\n--------------\\n\")\n",
        "x = x.reshape((2,3))\n",
        "print(x)\n",
        "print(x.shape,end=\"\\n--------------\\n\")\n",
        "\"\"\"\n",
        "burada\n",
        "0 2 4\n",
        "1 3 5\n",
        "\n",
        "şeklinde transpoz almak yerine\n",
        "\n",
        "0 1 2\n",
        "3 4 5\n",
        "\n",
        "geldi çünkü reshape öncelikle hepsini tek bir satıra alıyor \"flatten\" yapıyor\n",
        "[0 1 2 3 4 5] oluyor ve sonrasında ayırıyor\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "Du9Dz7hLIoQr",
        "outputId": "f0fdd21d-ddc7-4097-9496-582b31ea40ea"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2)\n",
            "--------------\n",
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [5.]]\n",
            "(6, 1)\n",
            "--------------\n",
            "[[0. 1. 2.]\n",
            " [3. 4. 5.]]\n",
            "(2, 3)\n",
            "--------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nburada \\n0 2 4 \\n1 3 5\\n\\nşeklinde transpoz almak yerine \\n\\n0 1 2 \\n3 4 5\\n\\ngeldi çünkü reshape öncelikle hepsini tek bir satıra alıyor \"flatten\" yapıyor\\n[0 1 2 3 4 5] oluyor ve sonrasında ayırıyor\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.zeros((300, 20))\n",
        "x = np.transpose(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tlkn2KGI4I8",
        "outputId": "ed88dd9f-5d4d-40a3-e810-95a1c6259611"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([\n",
        "    [0., 1.],\n",
        "    [2., 3.],\n",
        "    [4., 5.]\n",
        "])\n",
        "\n",
        "print(x,end=\"\\n------------\\n\")\n",
        "\n",
        "print(np.transpose(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY4_DxASK7Zm",
        "outputId": "c57eb527-7563-40e9-92c7-7997c4c504f5"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1.]\n",
            " [2. 3.]\n",
            " [4. 5.]]\n",
            "------------\n",
            "[[0. 2. 4.]\n",
            " [1. 3. 5.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTdjjicfLDoc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}